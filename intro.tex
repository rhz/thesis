% \input{epigraph}
% \noindent
\section{Historical background}
In the history of natural sciences,
there has been two main approaches to describe dynamical systems,
which I call here
\emph{kinetics} and \emph{thermodynamics}.
Loosely speaking, in the kinetic approach the system is
described by the positions and momenta of each particle.
This approach goes all the way back to
Newton's laws of motion~\citep{newton}.
Intuitively, it is a ``ground'' description in the sense that
it is as explicit and detailed as possible.
If a (classical) mechanical system has $N$ particles
then a state of the system is described by
a vector in $\RR^{2 \cdot 3 \cdot N}$.
% Regarding the role of forces in Newtonian mechanics:
% Interactions producing a change of momentum on a particle
% that can be measured independently do not interact between them
% and thus the resulting derivative of the momentum (force)
% is just the sum of the forces as measured independently.

On the other hand,
the thermodynamic approach shows how
all information about the change of the system in time
is contained in the energy function (for conservative systems).
Given a set of states $\states$ for the system,
an energy function $E: \states \to \RR$ maps a state to its energy.
In this way a state is described by a single scalar
regardless of how many particles it comprises.
Naturally, this approach endowed the description of
a dynamical system in classical mechanics
with a remarkable conciseness, simplicity and elegance.
It first appeared in the work of
\citet{lagrange2} and \citet{hamilton},
and has been subsequently used as the basis for most of modern physics.
Once in possession of the energy function,
the kinetic description (\ie the equations of motion)
can then be derived from it.
However the converse is not true:
in general a kinetic description might not have an energy function
from which it can be derived \citep{santilli},
partly because of non-conservative (\eg dissipative) forces.
Obtaining an energy function from the equations of motion
is called the \emph{inverse problem} in classical mechanics
and it was first attended to by \citet{helmholtz}.
Both the direct and the inverse problem are the interest of this thesis.
Note that this approach has been given the name `thermodynamic'
not because of thermodynamics,
the science that studies the dynamics of heat and temperature,
but because of the protagonical role of the energy
in driving the system's evolution.
Certainly, there are connections to thermodynamics
that will be highlighted as they arise.

Half a century after Hamilton's work
researchers like Maxwell, Boltzmann, and Gibbs
applied the ideas of classical mechanics to \emph{atoms}
in order to describe physical properties of matter like pressure,
the capacity to transfer heat, and others.
This body of work came to be known as \emph{statistical mechanics}
and was used to explain Brownian motion by \citet{einstein-brownian},
which after its experimental verification \citep{perrin}
settled the debate about the existence of atoms.
This work however did not attempt to explain
the chemical interactions and reactions that molecules undergo.
That would have to wait yet half a century
for the axiomatisation of probability theory by \citet{kolmogorov}
and the further developments by \citet{doob} and \citet{feller},
who, among others, established the theoretical framework
for continuous-time Markov chains (CTMCs).
Below you can find the definitions for a CTMC
and its infinitesimal generator that will be used in this thesis.
In particular, we work with time-homogeneous CTMCs.

\begin{definition}[Infinitesimal generator]
  An \emph{infinitesimal generator} $\qm$
  on an at most countable set of states $\states$
  is an $\states \times \states$ matrix
  with elements $q_{ij} \in \RR$, $i,j \in \states$
  such that $0 \leqslant q_{ij} < \infty$ when $i \neq j$
  and $q_{ii} = - \sum_{j \neq i} q_{ij} < \infty$.
\end{definition}

The infinitesimal generator plays the role of
the time derivative of the transition probabilities at time $0$
and induces the evolution of a probabilistic state
according to the Kolmogorov backward equation,
\begin{equation}
  \label{eq:transition-function}
  \ddt P(t) = Q P(t), \quad P(0) = I
\end{equation}
where $P(t)$ is the $\states \times \states$ matrix
with elements $p_{ij}(t) \in \RR$ the probability that
we were in state $i$ at time $0$ and are in state $j$ at time $t$.
% transition probability from state $i$ to $j$ at time $t$.
When the infinitesimal generator is stable and conservative
there is a unique minimal solution to \eqn{transition-function}
\citep{anderson}.
We shall work with this type of infinitesimal generators
and assume there is a transition function $P(t)$
whenever we have an infinitesimal generator $\qm$ and vice versa.

Given a \pmf $s(0)$ on $\states$ (seen as a row vector)
as an initial probabilistic state,
the probability distribution $s(t)$ after time $t$
is given by $s(t) = s(0) P(t)$.
% We can obtain the time derivative of this distribution
% from \eqn{transition-function}.
% % $s_i(t)$ that the Markov chain is in state $i$ at time $t$.
% In coordinate form, we have
% \begin{equation} % TODO: is this equation correct?
%   \label{eq:prob-deriv}
%   \ddt s_i(t) = \sum_{j \in \states} q_{ji} s_j(t)
% \end{equation}
We say the infinitesimal generator is \emph{irreducible}
if every state is reachable regardless of the initial state,
\ie $p_{ij}(t) > 0$ for all $i,j \in \states$
and some $t \geqslant 0$.

\begin{definition}[CTMC]%[continuous-time Markov chain]
  A \emph{continuous-time Markov chain} is a tuple
  $\tuple{\states, s(0), \qm}$ with
  $\states$ an at most countable set of states,
  $s(0)$ a \pmf on $\states$
  representing the initial probabilistic state and
  $\qm$ the infinitesimal generator of the Markov chain.
  % The Markov chain can be presented as a time-indexed family
  % of random variables $X_t$.
\end{definition}

% TODO: Am I here introducing something after using/mentioning it?
An important property of CTMCs for the present work is that of
\emph{time reversibility}, also known as \emph{detailed balance}.

\begin{definition}
  An infinitesimal generator $\qm$ on $\states$
  is said to be \emph{time reversible} iff
  there is a \pmf $\ip$ on $\states$ such that
  \begin{equation}
    \label{eq:detailed-balance}
    \ip_i q_{ij} = \ip_j q_{ji}
  \end{equation}
  for all $i,j \in \states$.
  Then $\qm$ is said to have \emph{detailed balance}
  with respect to $\ip$.
\end{definition}

This property is related to the existence of an \emph{invariant}
probability measure for the infinitesimal generator.

\begin{definition}
  A \pmf $\ip$ on $\states$ is
  \emph{invariant} for an infinitesimal generator $\qm$
  iff $\ip \qm = 0$, \ie
  \[ -\ip_i q_{ii} = \ip_i \sum_{j \neq i} q_{ij}
                  = \sum_{j \neq i} \ip_j q_{ji} \]
  In other words,
  whenever the action of $\qm$ on it does not change it.
\end{definition}

\begin{lemma}
  Suppose the infinitesimal generator $\qm$
  has detailed balance with respect to $\ip$.
  Then $\ip$ is invariant for $\qm$.
\end{lemma}
\begin{proof}
  From \eqn{detailed-balance} we obtain
  \[ \sum_{i \in \states} \ip_i q_{ij} =
     \sum_{i \in \states} \ip_j q_{ji} = -\ip_j q_{jj}, \]
  as $\sum_{i \in \states} q_{ji} = -q_{jj}$ for any fixed state $j$.
\end{proof}

Moreover, whenever an infinitesimal generator
is irreducible and has detailed balance with respect to $\ip$,
it will converge to this probability measure as $t \to \infty$.
This property is called \emph{ergodicity}.

\begin{lemma}
  Suppose $\qm$ is irreducible and
  has detailed balance with respect to $\ip$.
  Then it converges to $\ip$ as $t \to \infty$, \ie
  \[ \lim_{t \to \infty} s(0) P(t) = \ip \]
  for any $s(0)$.
\end{lemma}
\begin{proof}
\end{proof}

Metropolis and Hastings

The theory of CTMCs allows one to frame the dynamics of
chemical reaction systems.
The physical validity of using this stochastic approach
was argued by \citet{gillespie76}.
% However, we should first survey two contributions
% that precede Gillespie's work and are important
% for our presentation.

Petri

A formal language to describe chemical reaction systems
% came to be by the hand of Petri
was invented by \citet{petri}.
This language, later called Petri nets,
would define a reaction as a transformation of
multisets of chemical species.

However, Petri nets as a language for describing chemical reactions
has limitations when we take into consideration
how chemistry actually works.
When we look at what happens during chemical reactions
inside molecules, the chemical transformation amounts to
a change in the way electrons are shared by atoms
resulting in a relocation of chemical bonds.
It's all about atom binding and unbinding,
establishing connections and breaking them.
This is poorly captured by a change  of identity
as it is modelled in Petri nets.

More recently,
a formal language to describe biochemical interactions
where molecules not just react but can also bind other molecules
non-covalently has been created by \citet{danoslaneve2002a}.

















%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:

