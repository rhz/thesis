% \input{epigraph}
% \noindent
\section{Historical background}
In the history of natural sciences,
there has been two main approaches to describe dynamical systems,
which I call here
\emph{Kinetics} and \emph{Thermodynamics}.
Loosely speaking, in the kinetic approach the system is
described by the positions and momenta of each particle.
This approach goes all the way back to
Newton's laws of motion~\citep{newton}.
Intuitively, it is a ``ground'' description in the sense that
it is as explicit and detailed as possible.
If a (classical) mechanical system has $N$ particles
then a state of the system is described by
a vector in $\RR^{2 \cdot 3 \cdot N}$.
% Regarding the role of forces in Newtonian mechanics:
% Interactions producing a change of momentum on a particle
% that can be measured independently do not interact between them
% and thus the resulting derivative of the momentum (force)
% is just the sum of the forces as measured independently.

On the other hand,
the thermodynamic approach shows how
all information about the change of the system in time
is contained in the energy function (for conservative systems).
Given a set of states $\states$ for the system,
an energy function $E: \states \to \RR$ maps a state to its energy.
In this way a state is described by a single scalar
regardless of how many particles it comprises.
Naturally, this approach endowed the description of
a dynamical system in classical mechanics
with a remarkable conciseness, simplicity and elegance.
It first appeared in the work of
\citet{lagrange2} and \citet{hamilton},
and has been subsequently used as the basis for most of modern physics.
Once in possession of the energy function,
the kinetic description (\ie the equations of motion)
can then be derived from it.
However the converse is not true:
in general a kinetic description might not have an energy function
from which it can be derived \citep{santilli},
partly because of non-conservative (\eg dissipative) forces.
Obtaining an energy function from the equations of motion
is called the \emph{inverse problem} in classical mechanics
and it was first attended to by \citet{helmholtz}.
Both the direct and the inverse problem are the interest of this thesis.
Note that this approach has been given the name `thermodynamic'
not because of thermodynamics,
the science that studies the dynamics of heat and temperature,
but because of the protagonical role of the energy
in driving the system's evolution.
Certainly, there are connections to thermodynamics
that will be highlighted as they arise.

Half a century after Hamilton's work
researchers like Maxwell, Boltzmann, and Gibbs % several researchers
applied the ideas of classical mechanics to \emph{atoms}
in order to describe physical properties of matter like pressure,
the capacity to transfer heat, and others.
This body of work came to be known as \emph{statistical mechanics}
and was used to explain Brownian motion by \citet{einstein-brownian},
which after its experimental verification \citep{perrin}
settled the debate about the existence of atoms.
This work however did not attempt to explain
the chemical interactions and reactions that molecules undergo.
That would have to wait yet half a century
for the axiomatisation of probability theory by \citet{kolmogorov}
and the further developments by \citet{doob} and \citet{feller},
who, among others, established the theoretical framework
for continuous-time Markov chains (CTMCs).
Below you can find the definitions for a CTMC
and its infinitesimal generator that will be used in this thesis.

\begin{definition}[Infinitesimal generator]
  An \emph{infinitesimal generator} $\qm$
  on an at most countable set of states $\states$
  is an $\states \times \states$ matrix
  with elements $q_{ij} \in \RR$ ($i,j \in \states$)
  such that $0 \leqslant q_{ij} < \infty$ when $i \neq j$
  and $q_{ii} = - \sum_{j \neq i} q_{ij} < \infty$.
\end{definition}

\begin{definition}[CTMC]%[continuous-time Markov chain]
  A \emph{continuous-time Markov chain} is a tuple
  $\tuple{\states, p_0, \qm}$ with
  $\states$ an at most countable set of states,
  $p_0$ a probability mass function on $\states$
  representing the initial probabilistic state and
  $\qm$ the infinitesimal generator of the Markov chain.
  % The Markov chain can be presented as a time-indexed family
  % of random variables $X_t$.
\end{definition}

% TODO: maybe say a few words about how the infinitesimal generator
% is the time derivative of the jump probability and how it induces
% the evolution of the initial probabilistic state.

An important property of CTMCs for the present work is that of
\emph{time reversibility}, also known as \emph{detailed balance}.

\begin{definition}
  An infinitesimal generator $\qm$ on $\states$
  is said to be \emph{time reversible} iff
  there is a probability mass function $\ip$ on $\states$ such that
  \begin{equation}
    \label{eq:detailed-balance}
    \ip_i q_{ij} = \ip_j q_{ji}
  \end{equation}
  Then $\qm$ is said to have \emph{detailed balance}
  with respect to $\ip$.
\end{definition}

This property is related to the existence of a
\emph{stationary} probability distribution for the Markov chain.

\begin{definition}
  A probability mass function $\ip$ on $\states$ is
  a \emph{stationary probability distribution} of
  the infinitesimal generator $\qm$ iff $\ip \qm = 0$, \ie
  \[ -\ip_i q_{ii} = \ip_i \sum_{j \neq i} q_{ij}
                  = \sum_{j \neq i} \ip_j q_{ji} \]
\end{definition}

\begin{lemma}
  Suppose the infinitesimal generator $\qm$
  has detailed balance with respect to $\ip$
  and is irreducible, \ie ... % FIXME
  Then $\qm$ has a unique stationary probability distribution $\ip$.
\end{lemma}
\begin{proof}
  From \eqn{detailed-balance} we obtain
  \[ \sum_{i \in \states} \ip_i q_{ij} =
     \sum_{i \in \states} \ip_j q_{ji} = -\ip_j q_{jj}, \]
  as $\sum_{i \in \states} q_{ji} = -q_{jj}$ for any fixed state $j$.
\end{proof}

Moreover, whenever a CTMC has detailed balance,
it will converge to its unique stationary probability distribution
as $t \to \infty$ (given some extra assumptions, see below).

\begin{lemma}
  Let $\tuple{\states, \rho, \qm}$ be a CTMC.
  Suppose $\qm$ has detailed balance and is irreducible.
  If $\qm$ is aperiodic, that is,
  if $\qm$ ..., % FIXME
  then it converges to its unique stationary distribution $\ip$.
  \[ \lim_{t \to \infty} \rho P_t = \ip \]
\end{lemma}
\begin{proof}
  The time derivative of the probability $s_i$ that the Markov chain
  is in state $i$ at time $t$ is
  \[ \ddt s_i(t) = \sum_{j \in \states} q_{ji}s_j(t) \]
\end{proof}

The theory of CTMCs allows one to frame the dynamics of
chemical reaction systems.
The physical validity of using this stochastic approach
was argued by \citet{gillespie76}.
% However, we should first survey two contributions
% that precede Gillespie's work and are important
% for our presentation.

Metropolis and Hastings

Petri

A formal language to describe chemical reaction systems
% came to be by the hand of Petri
was invented by \citet{petri}.
This language, later called Petri nets,
would define a reaction as a transformation of
multisets of chemical species.

However, Petri nets as a language for describing chemical reactions
has limitations when we take into consideration
how chemistry actually works.
When we look at what happens during chemical reactions
inside molecules, the chemical transformation amounts to
a change in the way electrons are shared by atoms
resulting in a relocation of chemical bonds.
It's all about atom binding and unbinding,
establishing connections and breaking them.
This is poorly captured by a change  of identity
as it is modelled in Petri nets.

More recently,
a formal language to describe biochemical interactions
where molecules not just react but can also bind other molecules
non-covalently has been created by \citet{danoslaneve2002a}.

















%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:

