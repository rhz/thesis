% \input{epigraph}
% \noindent
\section{Historical background}
In the history of natural sciences,
there has been two main approaches to describe dynamical systems,
which I call here
\emph{kinetics} and \emph{thermodynamics}.
Loosely speaking, in the kinetic approach the system is
described by the positions and momenta of each particle.
This approach goes all the way back to
Newton's laws of motion~\citep{newton}.
Intuitively, it is a ``ground'' description in the sense that
it is as explicit and detailed as possible.
If a (classical) mechanical system has $N$ particles
then a state of the system is described by
a vector in $\RR^{2 \cdot 3 \cdot N}$.
% Regarding the role of forces in Newtonian mechanics:
% Interactions producing a change of momentum on a particle
% that can be measured independently do not interact between them
% and thus the resulting derivative of the momentum (force)
% is just the sum of the forces as measured independently.

On the other hand,
the thermodynamic approach shows how
all information about the change of the system in time
is contained in the energy function (for conservative systems).
Given a set of states $\states$ for the system,
an energy function $E: \states \to \RR$ maps a state to its energy.
In this way a state is described by a single scalar
regardless of how many particles it comprises.
Naturally, this approach endowed the description of
a dynamical system in classical mechanics
with a remarkable conciseness, simplicity and elegance.
It first appeared in the work of
\citet{lagrange2} and \citet{hamilton},
and has been subsequently used as the basis for most of modern physics.
Once in possession of the energy function,
the kinetic description (\ie the equations of motion)
can then be derived from it.
However the converse is not true:
in general a kinetic description might not have an energy function
from which it can be derived \citep{santilli},
partly because of non-conservative (\eg dissipative) forces.
Obtaining an energy function from the equations of motion
is called the \emph{inverse problem} in classical mechanics
and it was first attended to by \citet{helmholtz}.
Both the direct and the inverse problem are the interest of this thesis.
Note that this approach has been given the name `thermodynamic'
not because of thermodynamics,
the science that studies the dynamics of heat and temperature,
but because of the protagonical role of the energy
in driving the system's evolution.
Certainly, there are connections to thermodynamics
that will be highlighted as they arise.

Half a century after Hamilton's work
researchers like Maxwell, Boltzmann, and Gibbs
applied the ideas of classical mechanics to \emph{atoms}
in order to describe physical properties of matter like pressure,
the capacity to transfer heat, and others.
This body of work came to be known as \emph{statistical mechanics}
and was used to explain Brownian motion by \citet{einstein-brownian},
which after its experimental verification \citep{perrin}
settled the debate about the existence of atoms.
This work however did not attempt to explain
the chemical interactions and reactions that molecules undergo.
That would have to wait yet half a century
for the axiomatisation of probability theory by \citet{kolmogorov}
and the further developments by \citet{doob} and \citet{feller},
who, among others, established the theoretical framework
for continuous-time Markov chains (CTMCs).
Below you can find the definitions for a CTMC
and its infinitesimal generator that will be used in this thesis.
In particular, we work with time-homogeneous CTMCs.

\begin{definition}%[Infinitesimal generator]
  An \emph{infinitesimal generator} $\qm$
  on an at most countable set of states $\states$
  is an $\states \times \states$ matrix
  with elements $q_{ij} \in \RR$, $i,j \in \states$
  such that $0 \leqslant q_{ij} < \infty$ when $i \neq j$
  and $q_{ii} = - \sum_{j \neq i} q_{ij} < \infty$.
\end{definition}

The infinitesimal generator plays the role of
the time derivative of the transition probabilities at time $0$
and induces the evolution of a probabilistic state
according to the Kolmogorov backward equation,
\begin{equation}
  \label{eq:transition-function}
  \ddt P(t) = Q P(t), \quad P(0) = I
\end{equation}
where $P(t)$ is the $\states \times \states$ matrix
with elements $p_{ij}(t) \in \RR$ the probability that
we were in state $i$ at time $0$ and are in state $j$ at time $t$.
% transition probability from state $i$ to $j$ at time $t$.
When the infinitesimal generator is stable and conservative
there is a unique minimal solution to \eqn{transition-function}
\citep{anderson}.
We shall work with this type of infinitesimal generators
and assume there is a transition function $P(t)$
whenever we have an infinitesimal generator $\qm$ and vice versa.

Given a \pmf $s(0)$ on $\states$ (seen as a row vector)
as an initial probabilistic state,
the probability distribution $s(t)$ after time $t$
is given by $s(t) = s(0) P(t)$.
% We can obtain the time derivative of this distribution
% from \eqn{transition-function}.
% % $s_i(t)$ that the Markov chain is in state $i$ at time $t$.
% In coordinate form, we have
% \begin{equation} % TODO: is this equation correct?
%   \label{eq:prob-deriv}
%   \ddt s_i(t) = \sum_{j \in \states} q_{ji} s_j(t)
% \end{equation}
We say the infinitesimal generator is \emph{irreducible}
if every state is reachable regardless of the initial state,
\ie $p_{ij}(t) > 0$ for all $i,j \in \states$
and some $t \geqslant 0$.

\begin{definition}[CTMC]%[continuous-time Markov chain]
  A \emph{continuous-time Markov chain} is a tuple
  $\tuple{\states, s(0), \qm}$ with
  $\states$ an at most countable set of states,
  $s(0)$ a \pmf on $\states$
  representing the initial probabilistic state and
  $\qm$ the infinitesimal generator of the Markov chain.
  % The Markov chain can be presented as a time-indexed family
  % of random variables $X_t$.
\end{definition}

% TODO: Am I here introducing something after using/mentioning it?
An important property of CTMCs for the present work is that of
\emph{time reversibility}, also known as \emph{detailed balance},
which we introduce below.

\begin{definition}[detailed balance]
  An infinitesimal generator $\qm$ on $\states$
  is said to be \emph{time reversible} iff
  there is a \pmf $\ip$ on $\states$ such that
  \begin{equation}
    \label{eq:detailed-balance}
    \ip_i q_{ij} = \ip_j q_{ji}
  \end{equation}
  for all $i,j \in \states$.
  Then $\qm$ is said to have \emph{detailed balance}
  with respect to $\ip$.
\end{definition}

Intuitively, this property is related to the existence of an
\emph{invariant} probability measure for the infinitesimal generator.

\begin{definition}
  A \pmf $\ip$ on $\states$ is
  \emph{invariant} for an infinitesimal generator $\qm$
  iff $\ip \qm = 0$, \ie
  \[ -\ip_i q_{ii} = \ip_i \sum_{j \neq i} q_{ij}
                  = \sum_{j \neq i} \ip_j q_{ji} \]
  In other words,
  whenever the action of $\qm$ on it does not change it.
\end{definition}

\begin{lemma}
  Suppose the infinitesimal generator $\qm$
  has detailed balance with respect to $\ip$.
  Then $\ip$ is invariant for $\qm$.
\end{lemma}
\begin{proof}
  From \eqn{detailed-balance} we obtain
  \[ \sum_{i \in \states} \ip_i q_{ij} =
     \sum_{i \in \states} \ip_j q_{ji} = -\ip_j q_{jj}, \]
  as $\sum_{i \in \states} q_{ji} = -q_{jj}$ for any fixed state $j$.
\end{proof}

Moreover, we would like to know when this invariant
probability measure is realised by the Markov chain.

\begin{definition}[ergodicity]
  An infinitesimal generator $\qm$ is \emph{ergodic} when
  there is a probability measure $\ip$ on $\states$ such that
  \[ \lim_{t \to \infty} P_{ij}(t) = \ip_j \]
  for all $i,j \in \states$.
\end{definition}

This is equivalent to say that the Markov chain
will converge to the probability measure $\ip$
regardless of the initial state $s(0)$.

\begin{lemma}
  Suppose the infinitesimal generator $\qm$ is irreducible
  and has an invariant probability measure $\ip$.
  Then $\qm$ is ergodic and converges to $\ip$.
\end{lemma}

The proof for this lemma can be found in part 2 of theorem 1.6
in chapter 5 of Anderson's book (\cite*[][pages 160--161]{anderson}).

Say something about the kinetic feeling of CTMCs.

Metropolis and Hastings

The theory of CTMCs allows one to frame
the dynamics of chemical reaction systems.
However, since the number of molecules of a species
is a priori unbounded and thus $\states$ is infinite,
one would like to have a way to express these systems
in a finite and simple form.
A language that could do this % was invented by \citet{petri}.
came to be in the work of \citet{petri}.
This language, later called \emph{Petri nets},
sees reactions as transformations of
multisets of chemical species.

\begin{definition}
  A \emph{multiset} $M$ over a set $X$ is a map from $X$ to
  the positive integers assigning to each element $x \in X$
  the number of copies $M(x) \in \ZZ^+$ of that element
  in the multiset.
\end{definition}

\begin{definition}
  Given a set of species $\species$,
  a \emph{reaction} $r$ is a pair $\tuple{L,R}$
  with $L$ and $R$ multisets over $\species$.
  We refer to $L$ and $R$ as the left- and right-hand side of $r$
  and write $L \to R$ for the reaction.
\end{definition}

\begin{definition}%[PN]%[Petri net]
  A \emph{Petri net} is a tuple $\tuple{\species, \reactions, k}$
  with $\species$ a set of species,
  $\reactions$ a set of reactions and
  $k: \reactions \to \RR^+$ a kinetic map that assigns
  a rate $k(r)$ to each reaction $r \in \reactions$.
\end{definition}

The physical validity of this stochastic approach
together with the physical conditions under which
it can be used has been argued by \citet{gillespie76}.
% The physical conditions under which this stochastic approach
% is valid has been argued by \citet{gillespie76}.

% However,
Petri nets % as a language for describing chemical reactions
have limitations when we take into consideration
% how chemistry actually works.
% consider what takes place in chemical processes.
what happens inside molecules in a chemical reaction.
The chemical transformation taking place amounts to
a change in the way electrons are shared by atoms
resulting in a relocation of chemical bonds.
It is all about atom binding and unbinding,
\ie how they establish connections and break them.
This is poorly captured by a change of species,
as it is modelled by Petri nets.
A consequence of this lack of a formal representation for
molecular bonds is that certain systems of chemical reactions
cannot be described in a finite way using Petri nets,
\eg unbounded polymerisation.

Recently,
a formal language to describe biochemical interactions
where molecules not just react but can also bind other molecules
non-covalently has been created by \citet{danoslaneve2002a}.

















%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:

